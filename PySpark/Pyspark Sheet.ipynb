{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ðŸ ðŸ“„ PySpark Cheat Sheet\n",
    "\n",
    "A quick reference guide to the most commonly used patterns and functions in PySpark SQL.\n",
    "\n",
    "#### Table of Contents\n",
    "- [Quickstart](#quickstart)\n",
    "- [Basics](#basics)\n",
    "- [Common Patterns](#common-patterns)\n",
    "    - [Importing Functions & Types](#importing-functions--types)\n",
    "    - [Filtering](#filtering)\n",
    "    - [Joins](#joins)\n",
    "    - [Column Operations](#column-operations)\n",
    "    - [Casting & Coalescing Null Values & Duplicates](#casting--coalescing-null-values--duplicates)\n",
    "- [String Operations](#string-operations)\n",
    "    - [String Filters](#string-filters)\n",
    "    - [String Functions](#string-functions)\n",
    "- [Number Operations](#number-operations)\n",
    "- [Date & Timestamp Operations](#date--timestamp-operations)\n",
    "- [Array Operations](#array-operations)\n",
    "- [Struct Operations](#struct-operations)\n",
    "- [Aggregation Operations](#aggregation-operations)\n",
    "- [Advanced Operations](#advanced-operations)\n",
    "    - [Repartitioning](#repartitioning)\n",
    "    - [UDFs (User Defined Functions](#udfs-user-defined-functions)\n",
    "- [Useful Functions / Tranformations](#useful-functions--transformations)\n",
    "\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "Install on macOS:\n",
    "\n",
    "```bash\n",
    "brew install apache-spark && pip install pyspark\n",
    "```\n",
    "\n",
    "Create your first DataFrame:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# I/O options: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/io.html\n",
    "df = spark.read.csv('/path/to/your/input/file')\n",
    "```\n",
    "\n",
    "## Basics\n",
    "\n",
    "```python\n",
    "# Show a preview\n",
    "df.show()\n",
    "\n",
    "# Show preview of first / last n rows\n",
    "df.head(5)\n",
    "df.tail(5)\n",
    "\n",
    "# Show preview as JSON (WARNING: in-memory)\n",
    "df = df.limit(10) # optional\n",
    "print(json.dumps([row.asDict(recursive=True) for row in df.collect()], indent=2))\n",
    "\n",
    "# Limit actual DataFrame to n rows (non-deterministic)\n",
    "df = df.limit(5)\n",
    "\n",
    "# Get columns\n",
    "df.columns\n",
    "\n",
    "# Get columns + column types\n",
    "df.dtypes\n",
    "\n",
    "# Get schema\n",
    "df.schema\n",
    "\n",
    "# Get row count\n",
    "df.count()\n",
    "\n",
    "# Get column count\n",
    "len(df.columns)\n",
    "\n",
    "# Write output to disk\n",
    "df.write.csv('/path/to/your/output/file')\n",
    "\n",
    "# Get results (WARNING: in-memory) as list of PySpark Rows\n",
    "df = df.collect()\n",
    "\n",
    "# Get results (WARNING: in-memory) as list of Python dicts\n",
    "dicts = [row.asDict(recursive=True) for row in df.collect()]\n",
    "\n",
    "# Convert (WARNING: in-memory) to Pandas DataFrame\n",
    "df = df.toPandas()\n",
    "```\n",
    "\n",
    "## Common Patterns\n",
    "\n",
    "#### Importing Functions & Types\n",
    "\n",
    "```python\n",
    "# Easily reference these as F.my_function() and T.my_type() below\n",
    "from pyspark.sql import functions as F, types as T\n",
    "```\n",
    "\n",
    "#### Filtering\n",
    "\n",
    "```python\n",
    "# Filter on equals condition\n",
    "df = df.filter(df.is_adult == 'Y')\n",
    "\n",
    "# Filter on >, <, >=, <= condition\n",
    "df = df.filter(df.age > 25)\n",
    "\n",
    "# Multiple conditions require parentheses around each condition\n",
    "df = df.filter((df.age > 25) & (df.is_adult == 'Y'))\n",
    "\n",
    "# Compare against a list of allowed values\n",
    "df = df.filter(col('first_name').isin([3, 4, 7]))\n",
    "\n",
    "# Sort results\n",
    "df = df.orderBy(df.age.asc()))\n",
    "df = df.orderBy(df.age.desc()))\n",
    "```\n",
    "\n",
    "#### Joins\n",
    "\n",
    "```python\n",
    "# Left join in another dataset\n",
    "df = df.join(person_lookup_table, 'person_id', 'left')\n",
    "\n",
    "# Match on different columns in left & right datasets\n",
    "df = df.join(other_table, df.id == other_table.person_id, 'left')\n",
    "\n",
    "# Match on multiple columns\n",
    "df = df.join(other_table, ['first_name', 'last_name'], 'left')\n",
    "```\n",
    "\n",
    "#### Column Operations\n",
    "\n",
    "```python\n",
    "# Add a new static column\n",
    "df = df.withColumn('status', F.lit('PASS'))\n",
    "\n",
    "# Construct a new dynamic column\n",
    "df = df.withColumn('full_name', F.when(\n",
    "    (df.fname.isNotNull() & df.lname.isNotNull()), F.concat(df.fname, df.lname)\n",
    ").otherwise(F.lit('N/A'))\n",
    "\n",
    "# Pick which columns to keep, optionally rename some\n",
    "df = df.select(\n",
    "    'name',\n",
    "    'age',\n",
    "    F.col('dob').alias('date_of_birth'),\n",
    ")\n",
    "\n",
    "# Remove columns\n",
    "df = df.drop('mod_dt', 'mod_username')\n",
    "\n",
    "# Rename a column\n",
    "df = df.withColumnRenamed('dob', 'date_of_birth')\n",
    "\n",
    "# Keep all the columns which also occur in another dataset\n",
    "df = df.select(*(F.col(c) for c in df2.columns))\n",
    "\n",
    "# Batch Rename/Clean Columns\n",
    "for col in df.columns:\n",
    "    df = df.withColumnRenamed(col, col.lower().replace(' ', '_').replace('-', '_'))\n",
    "```\n",
    "\n",
    "#### Casting & Coalescing Null Values & Duplicates\n",
    "\n",
    "```python\n",
    "# Cast a column to a different type\n",
    "df = df.withColumn('price', df.price.cast(T.DoubleType()))\n",
    "\n",
    "# Replace all nulls with a specific value\n",
    "df = df.fillna({\n",
    "    'first_name': 'Tom',\n",
    "    'age': 0,\n",
    "})\n",
    "\n",
    "# Take the first value that is not null\n",
    "df = df.withColumn('last_name', F.coalesce(df.last_name, df.surname, F.lit('N/A')))\n",
    "\n",
    "# Drop duplicate rows in a dataset (distinct)\n",
    "df = df.dropDuplicates() # or\n",
    "df = df.distinct()\n",
    "\n",
    "# Drop duplicate rows, but consider only specific columns\n",
    "df = df.dropDuplicates(['name', 'height'])\n",
    "\n",
    "# Replace empty strings with null (leave out subset keyword arg to replace in all columns)\n",
    "df = df.replace({\"\": None}, subset=[\"name\"])\n",
    "\n",
    "# Convert Python/PySpark/NumPy NaN operator to null\n",
    "df = df.replace(float(\"nan\"), None)\n",
    "```\n",
    "\n",
    "## String Operations\n",
    "\n",
    "#### String Filters\n",
    "\n",
    "```python\n",
    "# Contains - col.contains(string)\n",
    "df = df.filter(df.name.contains('o'))\n",
    "\n",
    "# Starts With - col.startswith(string)\n",
    "df = df.filter(df.name.startswith('Al'))\n",
    "\n",
    "# Ends With - col.endswith(string)\n",
    "df = df.filter(df.name.endswith('ice'))\n",
    "\n",
    "# Is Null - col.isNull()\n",
    "df = df.filter(df.is_adult.isNull())\n",
    "\n",
    "# Is Not Null - col.isNotNull()\n",
    "df = df.filter(df.first_name.isNotNull())\n",
    "\n",
    "# Like - col.like(string_with_sql_wildcards)\n",
    "df = df.filter(df.name.like('Al%'))\n",
    "\n",
    "# Regex Like - col.rlike(regex)\n",
    "df = df.filter(df.name.rlike('[A-Z]*ice$'))\n",
    "\n",
    "# Is In List - col.isin(*cols)\n",
    "df = df.filter(df.name.isin('Bob', 'Mike'))\n",
    "```\n",
    "\n",
    "#### String Functions\n",
    "\n",
    "```python\n",
    "# Substring - col.substr(startPos, length)\n",
    "df = df.withColumn('short_id', df.id.substr(0, 10))\n",
    "\n",
    "# Trim - F.trim(col)\n",
    "df = df.withColumn('name', F.trim(df.name))\n",
    "\n",
    "# Left Pad - F.lpad(col, len, pad)\n",
    "# Right Pad - F.rpad(col, len, pad)\n",
    "df = df.withColumn('id', F.lpad('id', 4, '0'))\n",
    "\n",
    "# Left Trim - F.ltrim(col)\n",
    "# Right Trim - F.rtrim(col)\n",
    "df = df.withColumn('id', F.ltrim('id'))\n",
    "\n",
    "# Concatenate - F.concat(*cols)\n",
    "df = df.withColumn('full_name', F.concat('fname', F.lit(' '), 'lname'))\n",
    "\n",
    "# Concatenate with Separator/Delimiter - F.concat_ws(delimiter, *cols)\n",
    "df = df.withColumn('full_name', F.concat_ws('-', 'fname', 'lname'))\n",
    "\n",
    "# Regex Replace - F.regexp_replace(str, pattern, replacement)[source]\n",
    "df = df.withColumn('id', F.regexp_replace(id, '0F1(.*)', '1F1-$1'))\n",
    "\n",
    "# Regex Extract - F.regexp_extract(str, pattern, idx)\n",
    "df = df.withColumn('id', F.regexp_extract(id, '[0-9]*', 0))\n",
    "```\n",
    "\n",
    "## Number Operations\n",
    "\n",
    "```python\n",
    "# Round - F.round(col, scale=0)\n",
    "df = df.withColumn('price', F.round('price', 0))\n",
    "\n",
    "# Floor - F.floor(col)\n",
    "df = df.withColumn('price', F.floor('price'))\n",
    "\n",
    "# Ceiling - F.ceil(col)\n",
    "df = df.withColumn('price', F.ceil('price'))\n",
    "\n",
    "# Absolute Value - F.abs(col)\n",
    "df = df.withColumn('price', F.abs('price'))\n",
    "\n",
    "# X raised to power Y â€“ F.pow(x, y)\n",
    "df = df.withColumn('exponential_growth', F.pow('x', 'y'))\n",
    "\n",
    "# Select smallest value out of multiple columns â€“ F.least(*cols)\n",
    "df = df.withColumn('least', F.least('subtotal', 'total'))\n",
    "\n",
    "# Select largest value out of multiple columns â€“ F.greatest(*cols)\n",
    "df = df.withColumn('greatest', F.greatest('subtotal', 'total'))\n",
    "```\n",
    "\n",
    "## Date & Timestamp Operations\n",
    "\n",
    "```python\n",
    "# Add a column with the current date\n",
    "df = df.withColumn('current_date', F.current_date())\n",
    "\n",
    "# Convert a string of known format to a date (excludes time information)\n",
    "df = df.withColumn('date_of_birth', F.to_date('date_of_birth', 'yyyy-MM-dd'))\n",
    "\n",
    "# Convert a string of known format to a timestamp (includes time information)\n",
    "df = df.withColumn('time_of_birth', F.to_timestamp('time_of_birth', 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "# Get year from date:       F.year(col)\n",
    "# Get month from date:      F.month(col)\n",
    "# Get day from date:        F.dayofmonth(col)\n",
    "# Get hour from date:       F.hour(col)\n",
    "# Get minute from date:     F.minute(col)\n",
    "# Get second from date:     F.second(col)\n",
    "df = df.filter(F.year('date_of_birth') == F.lit('2017'))\n",
    "\n",
    "# Add & subtract days\n",
    "df = df.withColumn('three_days_after', F.date_add('date_of_birth', 3))\n",
    "df = df.withColumn('three_days_before', F.date_sub('date_of_birth', 3))\n",
    "\n",
    "# Add & Subtract months\n",
    "df = df.withColumn('next_month', F.add_month('date_of_birth', 1))\n",
    "\n",
    "# Get number of days between two dates\n",
    "df = df.withColumn('days_between', F.datediff('start', 'end'))\n",
    "\n",
    "# Get number of months between two dates\n",
    "df = df.withColumn('months_between', F.months_between('start', 'end'))\n",
    "\n",
    "# Keep only rows where date_of_birth is between 2017-05-10 and 2018-07-21\n",
    "df = df.filter(\n",
    "    (F.col('date_of_birth') >= F.lit('2017-05-10')) &\n",
    "    (F.col('date_of_birth') <= F.lit('2018-07-21'))\n",
    ")\n",
    "```\n",
    "\n",
    "## Array Operations\n",
    "\n",
    "```python\n",
    "# Column Array - F.array(*cols)\n",
    "df = df.withColumn('full_name', F.array('fname', 'lname'))\n",
    "\n",
    "# Empty Array - F.array(*cols)\n",
    "df = df.withColumn('empty_array_column', F.array([]))\n",
    "\n",
    "# Get element at index â€“Â col.getItem(n)\n",
    "df = df.withColumn('first_element', F.col(\"my_array\").getItem(0))\n",
    "\n",
    "# Array Size/Length â€“Â F.size(col)\n",
    "df = df.withColumn('array_length', F.size('my_array'))\n",
    "\n",
    "# Flatten Array â€“ F.flatten(col)\n",
    "df = df.withColumn('flattened', F.flatten('my_array'))\n",
    "\n",
    "# Unique/Distinct Elements â€“ F.array_distinct(col)\n",
    "df = df.withColumn('unique_elements', F.array_distinct('my_array'))\n",
    "\n",
    "# Map over & transform array elements â€“ F.transform(col, func: col -> col)\n",
    "df = df.withColumn('elem_ids', F.transform(F.col('my_array'), lambda x: x.getField('id')))\n",
    "\n",
    "# Return a row per array element â€“Â F.explode(col)\n",
    "df = df.select(F.explode('my_array'))\n",
    "```\n",
    "\n",
    "## Struct Operations\n",
    "\n",
    "```python\n",
    "# Make a new Struct column (similar to Python's `dict()`) â€“Â F.struct(*cols)\n",
    "df = df.withColumn('my_struct', F.struct(F.col('col_a'), F.col('col_b')))\n",
    "\n",
    "# Get item from struct by key â€“Â col.getField(str)\n",
    "df = df.withColumn('col_a', F.col('my_struct').getField('col_a'))\n",
    "```\n",
    "\n",
    "\n",
    "## Aggregation Operations\n",
    "\n",
    "```python\n",
    "# Row Count:                F.count()\n",
    "# Sum of Rows in Group:     F.sum(*cols)\n",
    "# Mean of Rows in Group:    F.mean(*cols)\n",
    "# Max of Rows in Group:     F.max(*cols)\n",
    "# Min of Rows in Group:     F.min(*cols)\n",
    "# First Row in Group:       F.alias(*cols)\n",
    "df = df.groupBy('gender').agg(F.max('age').alias('max_age_by_gender'))\n",
    "\n",
    "# Collect a Set of all Rows in Group:       F.collect_set(col)\n",
    "# Collect a List of all Rows in Group:      F.collect_list(col)\n",
    "df = df.groupBy('age').agg(F.collect_set('name').alias('person_names'))\n",
    "\n",
    "# Just take the lastest row for each combination (Window Functions)\n",
    "from pyspark.sql import Window as W\n",
    "\n",
    "window = W.partitionBy(\"first_name\", \"last_name\").orderBy(F.desc(\"date\"))\n",
    "df = df.withColumn(\"row_number\", F.row_number().over(window))\n",
    "df = df.filter(F.col(\"row_number\") == 1)\n",
    "df = df.drop(\"row_number\")\n",
    "```\n",
    "\n",
    "## Advanced Operations\n",
    "\n",
    "#### Repartitioning\n",
    "\n",
    "```python\n",
    "# Repartition â€“ df.repartition(num_output_partitions)\n",
    "df = df.repartition(1)\n",
    "```\n",
    "\n",
    "#### UDFs (User Defined Functions\n",
    "\n",
    "```python\n",
    "# Multiply each row's age column by two\n",
    "times_two_udf = F.udf(lambda x: x * 2)\n",
    "df = df.withColumn('age', times_two_udf(df.age))\n",
    "\n",
    "# Randomly choose a value to use as a row's name\n",
    "import random\n",
    "\n",
    "random_name_udf = F.udf(lambda: random.choice(['Bob', 'Tom', 'Amy', 'Jenna']))\n",
    "df = df.withColumn('name', random_name_udf())\n",
    "```\n",
    "\n",
    "## Useful Functions / Transformations\n",
    "\n",
    "```python\n",
    "def flatten(df: DataFrame, delimiter=\"_\") -> DataFrame:\n",
    "    '''\n",
    "    Flatten nested struct columns in `df` by one level separated by `delimiter`, i.e.:\n",
    "\n",
    "    df = [ {'a': {'b': 1, 'c': 2} } ]\n",
    "    df = flatten(df, '_')\n",
    "    -> [ {'a_b': 1, 'a_c': 2} ]\n",
    "    '''\n",
    "    flat_cols = [name for name, type in df.dtypes if not type.startswith(\"struct\")]\n",
    "    nested_cols = [name for name, type in df.dtypes if type.startswith(\"struct\")]\n",
    "\n",
    "    flat_df = df.select(\n",
    "        flat_cols\n",
    "        + [F.col(nc + \".\" + c).alias(nc + delimiter + c) for nc in nested_cols for c in df.select(nc + \".*\").columns]\n",
    "    )\n",
    "    return flat_df\n",
    "\n",
    "\n",
    "def lookup_and_replace(df1, df2, df1_key, df2_key, df2_value):\n",
    "    '''\n",
    "    Replace every value in `df1`'s `df1_key` column with the corresponding value\n",
    "    `df2_value` from `df2` where `df1_key` matches `df2_key`\n",
    "\n",
    "    df = lookup_and_replace(people, pay_codes, id, pay_code_id, pay_code_desc)\n",
    "    '''\n",
    "    return (\n",
    "        df1\n",
    "        .join(df2[[df2_key, df2_value]], df1[df1_key] == df2[df2_key], 'left')\n",
    "        .withColumn(df1_key, F.coalesce(F.col(df2_value), F.col(df1_key)))\n",
    "        .drop(df2_key)\n",
    "        .drop(df2_value)\n",
    "    )\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': 1, 'a': 2, 'y': 1, 'n': 2, 'k': 1, ' ': 1, 'S': 1, 'i': 1, 'g': 1, 'h': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k=\"Mayank Singh\"\n",
    "occurrences = {}\n",
    "\n",
    "for element in k:\n",
    "    if element in occurrences:\n",
    "        occurrences[element] += 1\n",
    "    else:\n",
    "        occurrences[element] = 1\n",
    "\n",
    "print(occurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
